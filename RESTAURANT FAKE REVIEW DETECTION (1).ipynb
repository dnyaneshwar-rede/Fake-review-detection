{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0445aa2-ced7-4cb0-8b36-ad5ff27f7987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\dnyan\\anaconda3\\lib\\site-packages (1.5.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\dnyan\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\dnyan\\anaconda3\\lib\\site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\dnyan\\anaconda3\\lib\\site-packages (from pandas) (1.23.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dnyan\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d3caa58-eef7-48b7-b611-2a38d36e9359",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:/Rastuarant Fake review detection/Jupyter/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab542ee5-53a7-4506-b856-267d1bce3fe9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set an appropriate chunk size based on your system's memory\n",
    "chunk_size = 1000\n",
    "\n",
    "# Initialize an empty list to store chunks\n",
    "chunks = []\n",
    "\n",
    "# Specify the path to your CSV file\n",
    "path = 'C:/Rastuarant Fake review detection/Jupyter/'\n",
    "\n",
    "# Iterate through chunks and append to the list\n",
    "for chunk in pd.read_csv(path + 'output_file_review.csv', chunksize=chunk_size):\n",
    "    chunks.append(chunk)\n",
    "\n",
    "# Concatenate the chunks into a single DataFrame\n",
    "csv_reader = pd.concat(chunks, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9365439a-9da0-4803-833a-426d90492e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_reader.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456b5933-ee1c-4b0f-8258-460ffa3c21f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_reader.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b4e28f-4351-44b6-b198-9777403307dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'csv_reader' is your DataFrame\n",
    "object_description = csv_reader.describe(include=['O'])  # 'O' stands for object data type\n",
    "\n",
    "# Display the descriptive statistics for object columns\n",
    "print(object_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88ae99c-648d-4e15-ae0e-a43fbf133624",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = csv_reader['review_id'].unique()\n",
    "unique_values_count = len(unique_values)\n",
    "\n",
    "print(\"Unique values in 'review_id' column:\", unique_values)\n",
    "print(\"Number of unique values:\", unique_values_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f17a84-ad53-4dae-97cc-64db9f00b355",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = csv_reader['business_id'].unique()\n",
    "unique_values_count = len(unique_values)\n",
    "\n",
    "print(\"Unique values in 'business_id' column:\", unique_values)\n",
    "print(\"Number of unique values:\", unique_values_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9572d988-8f1b-49f7-a4d4-77e5d5c7621c",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = csv_reader['user_id'].unique()\n",
    "unique_values_count = len(unique_values)\n",
    "\n",
    "print(\"Unique values in 'user_id' column:\", unique_values)\n",
    "print(\"Number of unique values:\", unique_values_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bd8221-d6d8-4055-bea3-fe0a1392f9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_reader = csv_reader.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92a18e5-8698-43bd-a7e9-785e30cac479",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove non-alphanumeric characters using regular expressions\n",
    "    clean_text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    return clean_text\n",
    "    #covert to lowercase\n",
    "    text = text.lower()\n",
    "csv_reader['text'][0], clean_text(csv_reader['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1763203-a4bb-40fc-99cb-b1f961ccce1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_reader['text'].head().apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1b7328-24e7-4019-afad-aa6e9c87f4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install wordcloud matplotlib numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808cc513-712d-48d1-9268-c5f3899de6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Sample text data (replace this with your Yelp dataset text)\n",
    "text_data = csv_reader['text']\n",
    "text_data = str(text_data)\n",
    "\n",
    "# Generate a word cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text_data)\n",
    "\n",
    "# Plot the WordCloud image\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')  # Turn off the axis labels\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2a813d-641a-42a6-b6e1-8512e98357a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "  return' '.join([word for word in word_tokenize(text) if word not in stopwords.words('english') and not word.isdigit() and word not in string.punctuation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d620655-efb8-4bff-8ec4-7684fb1e46da",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a59b70-4ac1-4ddf-9bf0-9a29ae8f8727",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "def clean_text(text):\n",
    "  csv_reader['tokens'] = csv_reader['clean_text'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cddfdb-99d2-4cd8-87a0-f6269bb753b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Sample text data (replace this with your Yelp dataset text)\n",
    "text_data = csv_reader['text']\n",
    "text_data = str(text_data)\n",
    "\n",
    "# Generate a word cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text_data)\n",
    "\n",
    "# Plot the WordCloud image\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')  # Turn off the axis labels\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e3287b-9a23-4fd3-aaee-25a3f00dee36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "  return ' '.join([word for word in word_tokenize(text) if word not in stopwords.words('english') and not word.isdigit() and word not in string.punctuation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc07ca45-b4a4-4812-bca9-9988cf975e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install swifter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c31455-0d2a-4a8c-90d3-13ab4778062e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8790a755-7bf1-41d0-8510-71d1dbceeead",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import nltk\n",
    "import swifter\n",
    "import spacy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2391d1-ab8c-43c9-84c8-e4aed7628cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download spaCy model\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "\n",
    "# Download NLTK resources (if not already downloaded)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740e3d2a-4a1a-4978-9865-e2506967aed4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Assuming csv_reader is your DataFrame\n",
    "def preprocess(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return ' '.join([word for word in word_tokenize(text) if word not in stop_words and not word.isdigit() and word not in string.punctuation])\n",
    "\n",
    "# Load the downloaded model\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "\n",
    "def preprocess_spacy(text):\n",
    "    if pd.isnull(text):\n",
    "        return ''\n",
    "    stop_words = set(stopwords.words('english'))  # Define stop_words here\n",
    "    doc = nlp(text)\n",
    "    return ' '.join([token.text for token in doc if token.text.lower() not in stop_words and not token.text.isdigit() and token.text.lower() not in string.punctuation])\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "csv_reader['cleaned_text'] = csv_reader['text'].swifter.apply(preprocess_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753f3f17-336a-47c1-924f-a40fc5fb390c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Sample text data (replace this with your Yelp dataset text)\n",
    "text_data = csv_reader['cleaned_text']\n",
    "text_data = str(text_data)\n",
    "\n",
    "# Generate a word cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text_data)\n",
    "\n",
    "# Plot the WordCloud image\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')  # Turn off the axis labels\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ec7387-9e8b-44ba-80c9-ac83c8413f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "def preprocess_with_stemming(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    return ' '.join([porter_stemmer.stem(word) for word in word_tokenize(text) if word not in stop_words and not word.isdigit() and word not in string.punctuation])\n",
    "\n",
    "# Example usage\n",
    "csv_reader['cleaned_text_stemmed'] = csv_reader['text'].apply(preprocess_with_stemming)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37fc9f6-3890-4832-bc7b-6ab78cbaeb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_reader.to_csv('Main_Project.csv', index=False)\n",
    "Main_Project_path = 'C:/Rastuarant Fake review detection/JupyterMain_Project.csv'\n",
    "csv_reader.to_csv(Main_Project_path, index=False)\n",
    "print(f\"Main_Project saved to {Main_Project_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1870e6c1-e67c-4252-b462-9697d18c7668",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ff5e3c-bf5d-43f1-a2f3-6ef6a3f3db89",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample list of text documents\n",
    "text=['text']\n",
    "# Create a TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b330ad88-a37c-4963-b671-2a181aec46e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pyspellchecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597fe770-811c-4e3f-b6b2-c85e9b82ab59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the full path to your preprocessed dataset\n",
    "file_path = 'C:/Rastuarant Fake review detection/Jupyter/Main_Project.csv'\n",
    "\n",
    "# Load the preprocessed dataset\n",
    "df_review = pd.read_csv(file_path)\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(df_review.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b49986a-b521-4ea1-9c58-3332a8fbf4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming you have a column named 'cleaned_text_stemmed'\n",
    "X = df_review['text']\n",
    "\n",
    "# Assuming 'is_fake' is your target variable\n",
    "y = df_review['is_fake']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65feadd1-8318-4a41-b072-ec2036f5e184",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load the preprocessed dataset\n",
    "df_review = pd.read_csv('C:/Users/Hp/Desktop/Spark Innovation/Jupyter/Main_Project.csv')\n",
    "# file_path = 'C:/Rastuarant Fake review detection/Jupyter/Main_Project.csv'\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(df_review.info())\n",
    "\n",
    "# Step 2: Feature Engineering\n",
    "\n",
    "# Criteria 1: Unusual language patterns (Example: Count of uppercase words)\n",
    "df_review['uppercase_word_count'] = df_review['text'].apply(lambda x: sum(1 for word in x.split() if word.isupper()))\n",
    "\n",
    "# Criteria 2: Abnormal review length\n",
    "df_review['review_length'] = df_review['text'].apply(len)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(df_review.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbba3c6-efe3-44bb-a6f1-f98f0e670ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Feature Engineering\n",
    "\n",
    "# Criteria 1: Unusual language patterns (Example: Count of uppercase words)\n",
    "df_review['uppercase_word_count'] = df_review['text'].apply(lambda x: sum(1 for word in x.split() if word.isupper()))\n",
    "\n",
    "# Criteria 2: Abnormal review length\n",
    "df_review['review_length'] = df_review['text'].apply(len)\n",
    "\n",
    "# Criteria 3: Suspicious timing or posting frequency (Assuming you have a 'timestamp' column)\n",
    "df_review['date'] = pd.to_datetime(df_review['date'])\n",
    "df_review['hour_of_day'] = df_review['date'].dt.hour\n",
    "\n",
    "# Criteria 4: Lack of details or specific information (Example: Count of numeric characters)\n",
    "df_review['numeric_char_count'] = df_review['text'].apply(lambda x: sum(1 for char in x if char.isdigit()))\n",
    "\n",
    "# Step 2: Explore the Features\n",
    "\n",
    "# Explore the new features\n",
    "print(df_review[['uppercase_word_count', 'review_length', 'hour_of_day', 'numeric_char_count']])\n",
    "\n",
    "# Step 3: Visualize the Features (Optional)\n",
    "\n",
    "# You can use various visualization libraries (e.g., seaborn, matplotlib) to explore the distribution of features.\n",
    "\n",
    "# Step 4: Analyze and Define Criteria\n",
    "\n",
    "# Based on the exploration, analyze the features and define criteria for identifying fake reviews.\n",
    "# For example, you might consider reviews with an unusually high count of uppercase words as suspicious.\n",
    "\n",
    "# Step 5: Apply the Criteria\n",
    "\n",
    "# Create a new column 'predicted_label' based on the defined criteria\n",
    "df_review['predicted_label'] = 0  # Initialize as 0\n",
    "# Apply criteria (Example: If uppercase_word_count > threshold, mark as fake)\n",
    "df_review.loc[df_review['uppercase_word_count'] > 5, 'predicted_label'] = 1  # Adjust the threshold as needed\n",
    "\n",
    "# Evaluate the results\n",
    "print(df_review[['text', 'predicted_label']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0093305f-bf3f-4dc3-844f-b866fd68a3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a DataFrame named df_review with a 'predicted_label' column\n",
    "\n",
    "# Create separate DataFrames for each label\n",
    "df_genuine = df_review[df_review['predicted_label'] == 0]\n",
    "df_fake = df_review[df_review['predicted_label'] == 1]\n",
    "\n",
    "# Save each DataFrame to a CSV file\n",
    "df_genuine.to_csv('True.csv', index=False)\n",
    "df_fake.to_csv('Fake.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a8e1b6-3a5b-4e96-8c0b-663d08a8e612",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV files into DataFrames\n",
    "df_genuine = pd.read_csv('True.csv')\n",
    "df_fake = pd.read_csv('Fake.csv')\n",
    "\n",
    "# Display basic information about the DataFrames\n",
    "print(\"Genuine Reviews:\")\n",
    "print(df_genuine.info())\n",
    "\n",
    "\n",
    "print(\"\\nFake Reviews:\")\n",
    "print(df_fake.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600f667c-0d52-4c2b-81e7-8d1e0048b4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrames to CSV files\n",
    "True_csv_path = 'C:/Rastuarant Fake review detection/Jupyter/True.csv'\n",
    "Fake_csv_path = 'C:/Rastuarant Fake review detection/Jupyter/Fake.csv'\n",
    "\n",
    "df_genuine.to_csv(True_csv_path, index=False)\n",
    "df_fake.to_csv(Fake_csv_path, index=False)\n",
    "\n",
    "print(f\"Genuine Reviews saved to {True_csv_path}\")\n",
    "print(f\"Fake Reviews saved to {Fake_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad9c02f0-20a2-475d-a542-f9d4a3389135",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48352ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:/Rastuarant Fake review detection/Jupyter/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95fc68e-04ba-4bd8-8ed3-9d677a44bf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fake = pd.read_csv(path + 'Fake.csv')\n",
    "data_true = pd.read_csv(path + 'True.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1dba5391-8b66-4609-9322-83ac89d2674b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>cleaned_text_stemmed</th>\n",
       "      <th>uppercase_word_count</th>\n",
       "      <th>review_length</th>\n",
       "      <th>hour_of_day</th>\n",
       "      <th>numeric_char_count</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lUUhg8ltDsUZ9h0xnwY4Dg</td>\n",
       "      <td>RreNy--tOmXMl1en0wiBOg</td>\n",
       "      <td>cPepkJeRMtHapc_b2Oe_dw</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>I was really between 3 and 4 stars for this on...</td>\n",
       "      <td>2018-07-17 03:30:07</td>\n",
       "      <td>really stars one LOVE 96th street Naked Tchops...</td>\n",
       "      <td>i realli star one i love 96th street nake tcho...</td>\n",
       "      <td>13</td>\n",
       "      <td>1555</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>p198qZsKOMCUhgdtRWsOKQ</td>\n",
       "      <td>3MpDvy5gEdsbZh9-p92dHg</td>\n",
       "      <td>8QnuWGVNBhzyYXGSeRdi4g</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>After my ROTD  yesterday of a different Sweet ...</td>\n",
       "      <td>2013-10-24 19:24:33</td>\n",
       "      <td>ROTD   yesterday different Sweet CeCe 's locat...</td>\n",
       "      <td>after rotd yesterday differ sweet cece 's loca...</td>\n",
       "      <td>7</td>\n",
       "      <td>490</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>YcLXh-3UC9y6YFAI9xxzPQ</td>\n",
       "      <td>G0DHgkSsDozqUPWtlxVEMw</td>\n",
       "      <td>oBhJuukGRqPVvYBfTkhuZA</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>The only reason I didn't give this restaurant ...</td>\n",
       "      <td>2015-03-05 03:37:54</td>\n",
       "      <td>reason n't give restaurant star rating one sin...</td>\n",
       "      <td>the reason i n't give restaur star rate one si...</td>\n",
       "      <td>8</td>\n",
       "      <td>2343</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cvQXRFLCyr0S7EgFb4lZqw</td>\n",
       "      <td>ZGjgfSvjQK886kiTzLwfLQ</td>\n",
       "      <td>EtKSTHV5Qx_Q7Aur9o4kQQ</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>On a scale of one to things that are awesome, ...</td>\n",
       "      <td>2009-10-14 01:15:04</td>\n",
       "      <td>scale one things awesome place bomb \\n\\n drawn...</td>\n",
       "      <td>on scale one thing awesom place bomb i drawn p...</td>\n",
       "      <td>7</td>\n",
       "      <td>934</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cejwyz1Hn1yQ-v2m9PAovQ</td>\n",
       "      <td>9ZLpaBGl-udvS-niR3Y_ow</td>\n",
       "      <td>GILL0ZkvVXJaNQzYRXfMQw</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>I absolutely love Courtney! She is hands down ...</td>\n",
       "      <td>2012-10-08 19:32:44</td>\n",
       "      <td>absolutely love Courtney hands best hairstylis...</td>\n",
       "      <td>i absolut love courtney she hand best hairstyl...</td>\n",
       "      <td>6</td>\n",
       "      <td>611</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                review_id                 user_id             business_id  \\\n",
       "0  lUUhg8ltDsUZ9h0xnwY4Dg  RreNy--tOmXMl1en0wiBOg  cPepkJeRMtHapc_b2Oe_dw   \n",
       "1  p198qZsKOMCUhgdtRWsOKQ  3MpDvy5gEdsbZh9-p92dHg  8QnuWGVNBhzyYXGSeRdi4g   \n",
       "2  YcLXh-3UC9y6YFAI9xxzPQ  G0DHgkSsDozqUPWtlxVEMw  oBhJuukGRqPVvYBfTkhuZA   \n",
       "3  cvQXRFLCyr0S7EgFb4lZqw  ZGjgfSvjQK886kiTzLwfLQ  EtKSTHV5Qx_Q7Aur9o4kQQ   \n",
       "4  cejwyz1Hn1yQ-v2m9PAovQ  9ZLpaBGl-udvS-niR3Y_ow  GILL0ZkvVXJaNQzYRXfMQw   \n",
       "\n",
       "   stars  useful  funny  cool  \\\n",
       "0      4       1      0     1   \n",
       "1      4       0      0     0   \n",
       "2      4       0      0     0   \n",
       "3      5       3      1     1   \n",
       "4      5       1      1     0   \n",
       "\n",
       "                                                text                 date  \\\n",
       "0  I was really between 3 and 4 stars for this on...  2018-07-17 03:30:07   \n",
       "1  After my ROTD  yesterday of a different Sweet ...  2013-10-24 19:24:33   \n",
       "2  The only reason I didn't give this restaurant ...  2015-03-05 03:37:54   \n",
       "3  On a scale of one to things that are awesome, ...  2009-10-14 01:15:04   \n",
       "4  I absolutely love Courtney! She is hands down ...  2012-10-08 19:32:44   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  really stars one LOVE 96th street Naked Tchops...   \n",
       "1  ROTD   yesterday different Sweet CeCe 's locat...   \n",
       "2  reason n't give restaurant star rating one sin...   \n",
       "3  scale one things awesome place bomb \\n\\n drawn...   \n",
       "4  absolutely love Courtney hands best hairstylis...   \n",
       "\n",
       "                                cleaned_text_stemmed  uppercase_word_count  \\\n",
       "0  i realli star one i love 96th street nake tcho...                    13   \n",
       "1  after rotd yesterday differ sweet cece 's loca...                     7   \n",
       "2  the reason i n't give restaur star rate one si...                     8   \n",
       "3  on scale one thing awesom place bomb i drawn p...                     7   \n",
       "4  i absolut love courtney she hand best hairstyl...                     6   \n",
       "\n",
       "   review_length  hour_of_day  numeric_char_count  predicted_label  \n",
       "0           1555            3                   8                1  \n",
       "1            490           19                   2                1  \n",
       "2           2343            3                  10                1  \n",
       "3            934            1                   5                1  \n",
       "4            611           19                   0                1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_fake.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b2d2e8e-c992-4870-ba54-a1aba72a4ee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>cleaned_text_stemmed</th>\n",
       "      <th>uppercase_word_count</th>\n",
       "      <th>review_length</th>\n",
       "      <th>hour_of_day</th>\n",
       "      <th>numeric_char_count</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KU_O5udG6zpxOg-VcAEodg</td>\n",
       "      <td>mh_-eMZ6K5RLWhZyISBhwA</td>\n",
       "      <td>XQfwVwDr-v0ZS3_CbbE5Xw</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>If you decide to eat here, just be aware it is...</td>\n",
       "      <td>2018-07-07 22:09:11</td>\n",
       "      <td>decide eat aware going take hours beginning en...</td>\n",
       "      <td>if decid eat awar go take hour begin end we tr...</td>\n",
       "      <td>3</td>\n",
       "      <td>513</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BiTunyQ73aT9WBnpR9DZGw</td>\n",
       "      <td>OyoGAe7OKpv6SyGZT5g77Q</td>\n",
       "      <td>7ATYjTIgM3jUlt4UM3IypQ</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>I've taken a lot of spin classes over the year...</td>\n",
       "      <td>2012-01-03 15:28:18</td>\n",
       "      <td>'ve taken lot spin classes years nothing compa...</td>\n",
       "      <td>i 've taken lot spin class year noth compar cl...</td>\n",
       "      <td>1</td>\n",
       "      <td>829</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>saUsX_uimxRlCVr67Z4Jig</td>\n",
       "      <td>8g_iMtfSiwikVnbP2etR0A</td>\n",
       "      <td>YjUWPpI6HXG530lwP-fb2A</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Family diner. Had the buffet. Eclectic assortm...</td>\n",
       "      <td>2014-02-05 20:30:30</td>\n",
       "      <td>Family diner buffet Eclectic assortment large ...</td>\n",
       "      <td>famili diner had buffet eclect assort larg chi...</td>\n",
       "      <td>0</td>\n",
       "      <td>339</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AqPFMleE6RsU23_auESxiA</td>\n",
       "      <td>_7bHUi9Uuf5__HHc_Q8guQ</td>\n",
       "      <td>kxX2SOes4o-D3ZQBkiMRfA</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Wow!  Yummy, different,  delicious.   Our favo...</td>\n",
       "      <td>2015-01-04 00:01:03</td>\n",
       "      <td>Wow   Yummy different   delicious    favorite ...</td>\n",
       "      <td>wow yummi differ delici our favorit lamb curri...</td>\n",
       "      <td>0</td>\n",
       "      <td>243</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sx8TMOWLNuJBWer-0pcmoA</td>\n",
       "      <td>bcjbaE6dDog4jkNY91ncLQ</td>\n",
       "      <td>e4Vwtrqf-wpJfwesgvdgxQ</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Cute interior and owner (?) gave us tour of up...</td>\n",
       "      <td>2017-01-14 20:54:15</td>\n",
       "      <td>Cute interior owner gave us tour upcoming pati...</td>\n",
       "      <td>cute interior owner gave us tour upcom patio/r...</td>\n",
       "      <td>2</td>\n",
       "      <td>534</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                review_id                 user_id             business_id  \\\n",
       "0  KU_O5udG6zpxOg-VcAEodg  mh_-eMZ6K5RLWhZyISBhwA  XQfwVwDr-v0ZS3_CbbE5Xw   \n",
       "1  BiTunyQ73aT9WBnpR9DZGw  OyoGAe7OKpv6SyGZT5g77Q  7ATYjTIgM3jUlt4UM3IypQ   \n",
       "2  saUsX_uimxRlCVr67Z4Jig  8g_iMtfSiwikVnbP2etR0A  YjUWPpI6HXG530lwP-fb2A   \n",
       "3  AqPFMleE6RsU23_auESxiA  _7bHUi9Uuf5__HHc_Q8guQ  kxX2SOes4o-D3ZQBkiMRfA   \n",
       "4  Sx8TMOWLNuJBWer-0pcmoA  bcjbaE6dDog4jkNY91ncLQ  e4Vwtrqf-wpJfwesgvdgxQ   \n",
       "\n",
       "   stars  useful  funny  cool  \\\n",
       "0      3       0      0     0   \n",
       "1      5       1      0     1   \n",
       "2      3       0      0     0   \n",
       "3      5       1      0     1   \n",
       "4      4       1      0     1   \n",
       "\n",
       "                                                text                 date  \\\n",
       "0  If you decide to eat here, just be aware it is...  2018-07-07 22:09:11   \n",
       "1  I've taken a lot of spin classes over the year...  2012-01-03 15:28:18   \n",
       "2  Family diner. Had the buffet. Eclectic assortm...  2014-02-05 20:30:30   \n",
       "3  Wow!  Yummy, different,  delicious.   Our favo...  2015-01-04 00:01:03   \n",
       "4  Cute interior and owner (?) gave us tour of up...  2017-01-14 20:54:15   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  decide eat aware going take hours beginning en...   \n",
       "1  've taken lot spin classes years nothing compa...   \n",
       "2  Family diner buffet Eclectic assortment large ...   \n",
       "3  Wow   Yummy different   delicious    favorite ...   \n",
       "4  Cute interior owner gave us tour upcoming pati...   \n",
       "\n",
       "                                cleaned_text_stemmed  uppercase_word_count  \\\n",
       "0  if decid eat awar go take hour begin end we tr...                     3   \n",
       "1  i 've taken lot spin class year noth compar cl...                     1   \n",
       "2  famili diner had buffet eclect assort larg chi...                     0   \n",
       "3  wow yummi differ delici our favorit lamb curri...                     0   \n",
       "4  cute interior owner gave us tour upcom patio/r...                     2   \n",
       "\n",
       "   review_length  hour_of_day  numeric_char_count  predicted_label  \n",
       "0            513           22                   1                0  \n",
       "1            829           15                   0                0  \n",
       "2            339           20                   0                0  \n",
       "3            243            0                   2                0  \n",
       "4            534           20                   0                0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_true.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e78c5f2e-3145-4efd-b316-21548b55a122",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fake[\"class\"] = 0\n",
    "data_true[\"class\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77ca4d27-6cac-4835-b8ac-f6387e259030",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1356207, 17), (5634073, 17))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_fake.shape, data_true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9d1f78b-4f0b-4deb-9cd1-db381216516e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fake_manual_testing = data_fake.tail(10)\n",
    "for i in range(1356206, 1356196, -1):\n",
    "    data_fake.drop([i], axis = 0, inplace = True) \n",
    "\n",
    "\n",
    "data_true_manual_testing = data_true.tail(10)\n",
    "for i in range(5634072, 5634062, -1):\n",
    "    data_true.drop([i], axis = 0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f55ea393-64da-441e-873e-1fe3d4d4f7f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1356197, 17), (5634063, 17))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_fake.shape, data_true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c84e4e7c-0598-4b0e-8f25-03e4f3b2c406",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dnyan\\AppData\\Local\\Temp\\ipykernel_29444\\277247672.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_fake_manual_testing['class'] = 0\n",
      "C:\\Users\\dnyan\\AppData\\Local\\Temp\\ipykernel_29444\\277247672.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_true_manual_testing['class'] = 1\n"
     ]
    }
   ],
   "source": [
    "data_fake_manual_testing['class'] = 0\n",
    "data_true_manual_testing['class'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b4ee018-f15a-4094-b4a5-1bbad5cd4958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>cleaned_text_stemmed</th>\n",
       "      <th>uppercase_word_count</th>\n",
       "      <th>review_length</th>\n",
       "      <th>hour_of_day</th>\n",
       "      <th>numeric_char_count</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1356197</th>\n",
       "      <td>X5R98ygOtbhryDiKA-J2qQ</td>\n",
       "      <td>LHWtjTG7e1NzNPYUbUo-9w</td>\n",
       "      <td>rgeuy1qbw6Z8B6CSVANHIA</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>I've been to the other Federal Donuts location...</td>\n",
       "      <td>2012-10-13 14:39:37</td>\n",
       "      <td>'ve Federal Donuts location multiple times lov...</td>\n",
       "      <td>i 've feder donut locat multipl time i love do...</td>\n",
       "      <td>9</td>\n",
       "      <td>752</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1356198</th>\n",
       "      <td>MVg4YUQeEhCA7Z7RsBJSVg</td>\n",
       "      <td>7-7A0Avj47slLGV7yBFc8w</td>\n",
       "      <td>ytynqOUb3hjKeJfRj5Tshw</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I was so excited about all the food I saw, but...</td>\n",
       "      <td>2013-07-25 21:00:15</td>\n",
       "      <td>excited food saw unfortunately place CLOSES EA...</td>\n",
       "      <td>i excit food i saw unfortun place close so ear...</td>\n",
       "      <td>8</td>\n",
       "      <td>262</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1356199</th>\n",
       "      <td>cOaCw12O2B3ER_Ir73w_bA</td>\n",
       "      <td>99_EbpwCcCRAa66YgorrKA</td>\n",
       "      <td>3MZ6DYP7EANJmP4UOW3PAw</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>The worst. Wish I had read all the other revie...</td>\n",
       "      <td>2016-04-27 04:14:52</td>\n",
       "      <td>worst Wish read reviews went   Car came dirty ...</td>\n",
       "      <td>the worst wish i read review i went car came d...</td>\n",
       "      <td>6</td>\n",
       "      <td>672</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1356200</th>\n",
       "      <td>99ylx-qPUSseITqBye2MpA</td>\n",
       "      <td>-AkziDwQ8hv2COTDBBUpig</td>\n",
       "      <td>aunmz06iWvo3bd6MMHEbqg</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Philly has become a dangerous place with the m...</td>\n",
       "      <td>2022-01-18 03:48:44</td>\n",
       "      <td>Philly become dangerous place murders per capi...</td>\n",
       "      <td>philli becom danger place murder per capita bi...</td>\n",
       "      <td>9</td>\n",
       "      <td>1433</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1356201</th>\n",
       "      <td>hCTngjYV495w9M-9sIOvXA</td>\n",
       "      <td>O5y7KlfBdqV6Ih8yhjZLsg</td>\n",
       "      <td>Oisn1dreLP_Ju344qlpWcQ</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>So I've belonged to a gym for YEARS and I fina...</td>\n",
       "      <td>2012-02-17 14:57:26</td>\n",
       "      <td>'ve belonged gym YEARS finallly learned someth...</td>\n",
       "      <td>so i 've belong gym year i finallli learn some...</td>\n",
       "      <td>10</td>\n",
       "      <td>742</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1356202</th>\n",
       "      <td>cACxcUY_AIsQKkpDRXuqnw</td>\n",
       "      <td>MCzlzlOw7IGbRAKVjJBPtg</td>\n",
       "      <td>fcGexL5VH5G2Xw0tRj9uOQ</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>This is a good pizza option - they deliver thr...</td>\n",
       "      <td>2018-03-13 13:54:48</td>\n",
       "      <td>good pizza option deliver GrubHub services   u...</td>\n",
       "      <td>thi good pizza option deliv grubhub servic i u...</td>\n",
       "      <td>6</td>\n",
       "      <td>791</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1356203</th>\n",
       "      <td>7NgXAuTFiJHYbuepOPwU0w</td>\n",
       "      <td>x1QLCwZGFAjxRRw4EHc3-g</td>\n",
       "      <td>1_BVWDzi5cVqWxNe9bOMMQ</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Don't misinterpret my 5-star review....I don't...</td>\n",
       "      <td>2016-04-30 01:02:34</td>\n",
       "      <td>n't misinterpret star review .... n't think We...</td>\n",
       "      <td>do n't misinterpret 5-star review .... i n't t...</td>\n",
       "      <td>11</td>\n",
       "      <td>890</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1356204</th>\n",
       "      <td>YVX1Wsa4LYxjvFwuHBb_gA</td>\n",
       "      <td>RKPkxOYQlM0BjhM-H6_vAw</td>\n",
       "      <td>X4mouE_cMiwbfyCPZ_K-FA</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Good, maybe very good.  I went for lunch, so t...</td>\n",
       "      <td>2015-01-23 23:52:03</td>\n",
       "      <td>Good maybe good   went lunch head chef n't rev...</td>\n",
       "      <td>good mayb good i went lunch head chef n't revi...</td>\n",
       "      <td>16</td>\n",
       "      <td>2700</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1356205</th>\n",
       "      <td>i-I4ZOhoX70Nw5H0FwrQUA</td>\n",
       "      <td>YwAMC-jvZ1fvEUum6QkEkw</td>\n",
       "      <td>Rr9kKArrMhSLVE9a53q-aA</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>For when I'm feeling like ignoring my calorie-...</td>\n",
       "      <td>2022-01-19 18:59:27</td>\n",
       "      <td>'m feeling like ignoring calorie counting indu...</td>\n",
       "      <td>for i 'm feel like ignor calorie-count indulg ...</td>\n",
       "      <td>9</td>\n",
       "      <td>2317</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1356206</th>\n",
       "      <td>RwcKOdEuLRHNJe4M9-qpqg</td>\n",
       "      <td>6JehEvdoCvZPJ_XIxnzIIw</td>\n",
       "      <td>VAeEXLbEcI9Emt9KGYq9aA</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>Located in the 'Walking District' in Nashville...</td>\n",
       "      <td>2018-01-02 22:50:47</td>\n",
       "      <td>Located Walking District Nashville 's bit way ...</td>\n",
       "      <td>locat 'walk district nashvil 's bit way us mis...</td>\n",
       "      <td>9</td>\n",
       "      <td>1198</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      review_id                 user_id  \\\n",
       "1356197  X5R98ygOtbhryDiKA-J2qQ  LHWtjTG7e1NzNPYUbUo-9w   \n",
       "1356198  MVg4YUQeEhCA7Z7RsBJSVg  7-7A0Avj47slLGV7yBFc8w   \n",
       "1356199  cOaCw12O2B3ER_Ir73w_bA  99_EbpwCcCRAa66YgorrKA   \n",
       "1356200  99ylx-qPUSseITqBye2MpA  -AkziDwQ8hv2COTDBBUpig   \n",
       "1356201  hCTngjYV495w9M-9sIOvXA  O5y7KlfBdqV6Ih8yhjZLsg   \n",
       "1356202  cACxcUY_AIsQKkpDRXuqnw  MCzlzlOw7IGbRAKVjJBPtg   \n",
       "1356203  7NgXAuTFiJHYbuepOPwU0w  x1QLCwZGFAjxRRw4EHc3-g   \n",
       "1356204  YVX1Wsa4LYxjvFwuHBb_gA  RKPkxOYQlM0BjhM-H6_vAw   \n",
       "1356205  i-I4ZOhoX70Nw5H0FwrQUA  YwAMC-jvZ1fvEUum6QkEkw   \n",
       "1356206  RwcKOdEuLRHNJe4M9-qpqg  6JehEvdoCvZPJ_XIxnzIIw   \n",
       "\n",
       "                    business_id  stars  useful  funny  cool  \\\n",
       "1356197  rgeuy1qbw6Z8B6CSVANHIA      5       1      1     1   \n",
       "1356198  ytynqOUb3hjKeJfRj5Tshw      3       1      0     0   \n",
       "1356199  3MZ6DYP7EANJmP4UOW3PAw      1       0      0     0   \n",
       "1356200  aunmz06iWvo3bd6MMHEbqg      3       0      0     0   \n",
       "1356201  Oisn1dreLP_Ju344qlpWcQ      5       2      0     1   \n",
       "1356202  fcGexL5VH5G2Xw0tRj9uOQ      3       1      1     0   \n",
       "1356203  1_BVWDzi5cVqWxNe9bOMMQ      5       1      0     1   \n",
       "1356204  X4mouE_cMiwbfyCPZ_K-FA      4       3      0     2   \n",
       "1356205  Rr9kKArrMhSLVE9a53q-aA      5       1      0     0   \n",
       "1356206  VAeEXLbEcI9Emt9KGYq9aA      3      10      3     7   \n",
       "\n",
       "                                                      text  \\\n",
       "1356197  I've been to the other Federal Donuts location...   \n",
       "1356198  I was so excited about all the food I saw, but...   \n",
       "1356199  The worst. Wish I had read all the other revie...   \n",
       "1356200  Philly has become a dangerous place with the m...   \n",
       "1356201  So I've belonged to a gym for YEARS and I fina...   \n",
       "1356202  This is a good pizza option - they deliver thr...   \n",
       "1356203  Don't misinterpret my 5-star review....I don't...   \n",
       "1356204  Good, maybe very good.  I went for lunch, so t...   \n",
       "1356205  For when I'm feeling like ignoring my calorie-...   \n",
       "1356206  Located in the 'Walking District' in Nashville...   \n",
       "\n",
       "                        date  \\\n",
       "1356197  2012-10-13 14:39:37   \n",
       "1356198  2013-07-25 21:00:15   \n",
       "1356199  2016-04-27 04:14:52   \n",
       "1356200  2022-01-18 03:48:44   \n",
       "1356201  2012-02-17 14:57:26   \n",
       "1356202  2018-03-13 13:54:48   \n",
       "1356203  2016-04-30 01:02:34   \n",
       "1356204  2015-01-23 23:52:03   \n",
       "1356205  2022-01-19 18:59:27   \n",
       "1356206  2018-01-02 22:50:47   \n",
       "\n",
       "                                              cleaned_text  \\\n",
       "1356197  've Federal Donuts location multiple times lov...   \n",
       "1356198  excited food saw unfortunately place CLOSES EA...   \n",
       "1356199  worst Wish read reviews went   Car came dirty ...   \n",
       "1356200  Philly become dangerous place murders per capi...   \n",
       "1356201  've belonged gym YEARS finallly learned someth...   \n",
       "1356202  good pizza option deliver GrubHub services   u...   \n",
       "1356203  n't misinterpret star review .... n't think We...   \n",
       "1356204  Good maybe good   went lunch head chef n't rev...   \n",
       "1356205  'm feeling like ignoring calorie counting indu...   \n",
       "1356206  Located Walking District Nashville 's bit way ...   \n",
       "\n",
       "                                      cleaned_text_stemmed  \\\n",
       "1356197  i 've feder donut locat multipl time i love do...   \n",
       "1356198  i excit food i saw unfortun place close so ear...   \n",
       "1356199  the worst wish i read review i went car came d...   \n",
       "1356200  philli becom danger place murder per capita bi...   \n",
       "1356201  so i 've belong gym year i finallli learn some...   \n",
       "1356202  thi good pizza option deliv grubhub servic i u...   \n",
       "1356203  do n't misinterpret 5-star review .... i n't t...   \n",
       "1356204  good mayb good i went lunch head chef n't revi...   \n",
       "1356205  for i 'm feel like ignor calorie-count indulg ...   \n",
       "1356206  locat 'walk district nashvil 's bit way us mis...   \n",
       "\n",
       "         uppercase_word_count  review_length  hour_of_day  numeric_char_count  \\\n",
       "1356197                     9            752           14                   0   \n",
       "1356198                     8            262           21                   2   \n",
       "1356199                     6            672            4                   0   \n",
       "1356200                     9           1433            3                  17   \n",
       "1356201                    10            742           14                   3   \n",
       "1356202                     6            791           13                   1   \n",
       "1356203                    11            890            1                   5   \n",
       "1356204                    16           2700           23                   3   \n",
       "1356205                     9           2317           18                   0   \n",
       "1356206                     9           1198           22                   0   \n",
       "\n",
       "         predicted_label  class  \n",
       "1356197                1      0  \n",
       "1356198                1      0  \n",
       "1356199                1      0  \n",
       "1356200                1      0  \n",
       "1356201                1      0  \n",
       "1356202                1      0  \n",
       "1356203                1      0  \n",
       "1356204                1      0  \n",
       "1356205                1      0  \n",
       "1356206                1      0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_fake_manual_testing.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0dcb82f8-0f1b-4b87-89f0-04f0aea3bfe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>cleaned_text_stemmed</th>\n",
       "      <th>uppercase_word_count</th>\n",
       "      <th>review_length</th>\n",
       "      <th>hour_of_day</th>\n",
       "      <th>numeric_char_count</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5634063</th>\n",
       "      <td>KlHxcAifUF5zDKpJCBrRsw</td>\n",
       "      <td>7ziWZULyiZv2TesYNMFf4g</td>\n",
       "      <td>qQO7ErS_RAN4Vs1uX0L55Q</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ice cream! ice cream sodas, sundaes!! \\n\\nwant...</td>\n",
       "      <td>2012-10-21 04:08:40</td>\n",
       "      <td>ice cream ice cream sodas sundaes \\n\\n wanted ...</td>\n",
       "      <td>ice cream ice cream soda sunda want share sund...</td>\n",
       "      <td>2</td>\n",
       "      <td>528</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5634064</th>\n",
       "      <td>PVRvzY0NxSU-fiK3JOXX7w</td>\n",
       "      <td>cTozFTTWjlFYc3yusdbZmA</td>\n",
       "      <td>uMVOtr16r1ELu46pWr4HCQ</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Just average Thai food tonight. Bangkok has al...</td>\n",
       "      <td>2022-01-18 06:42:59</td>\n",
       "      <td>average Thai food tonight Bangkok always go Th...</td>\n",
       "      <td>just averag thai food tonight bangkok alway go...</td>\n",
       "      <td>2</td>\n",
       "      <td>865</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5634065</th>\n",
       "      <td>E5sXn_qdPX5qRxUEApmiuA</td>\n",
       "      <td>Mc4C7fVY0sEcD-U5eOA2Og</td>\n",
       "      <td>ZNfph3_VzRLTOemk9Tmzaw</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>This gym gets the job done. \\n\\nUpsides: \\nChi...</td>\n",
       "      <td>2016-01-25 21:45:34</td>\n",
       "      <td>gym gets job done \\n\\n Upsides \\n Childcare in...</td>\n",
       "      <td>thi gym get job done upsid childcar includ mem...</td>\n",
       "      <td>1</td>\n",
       "      <td>1037</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5634066</th>\n",
       "      <td>MIXdpbbCTRAVdi8RiMjwdg</td>\n",
       "      <td>s67G457QlHSvk5RjOMN91w</td>\n",
       "      <td>58MJvmfo5hyfBbvkr54sFA</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Great gym.  Was in Indy for 4 days on business...</td>\n",
       "      <td>2022-01-18 15:24:44</td>\n",
       "      <td>Great gym   Indy days business local hotel gym...</td>\n",
       "      <td>great gym wa indi day busi local hotel gym mee...</td>\n",
       "      <td>4</td>\n",
       "      <td>388</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5634067</th>\n",
       "      <td>Sm8-QDsuQfik-QuhRYT5bw</td>\n",
       "      <td>QIXYkyAbTvgePUOH0-cRFg</td>\n",
       "      <td>Bh4b8wJRR_ggH7JvpCO7CQ</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>The front desk staff is very good and they hav...</td>\n",
       "      <td>2008-09-23 02:52:37</td>\n",
       "      <td>front desk staff good couple quality trainers ...</td>\n",
       "      <td>the front desk staff good coupl qualiti traine...</td>\n",
       "      <td>3</td>\n",
       "      <td>451</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5634068</th>\n",
       "      <td>wD5ZWao_vjyT2h4xmGam8Q</td>\n",
       "      <td>7L7GL5Pi2cf8mbm2Dpw4zw</td>\n",
       "      <td>e_E-jq9mwm7wk75k7Yi-Xw</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>It is very rare for a restaurant to be this go...</td>\n",
       "      <td>2022-01-17 22:36:01</td>\n",
       "      <td>rare restaurant good categories food ambience ...</td>\n",
       "      <td>it rare restaur good categori food ambienc ser...</td>\n",
       "      <td>0</td>\n",
       "      <td>336</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5634069</th>\n",
       "      <td>zHZ-A1qyKDEgyZMDaD--wg</td>\n",
       "      <td>_XVdmFWSgTN6YlojUxixTA</td>\n",
       "      <td>6WaI-IN8ql0xpEKlb4q8tg</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>We redesigned my moms dress and mad it complet...</td>\n",
       "      <td>2022-01-17 20:59:01</td>\n",
       "      <td>redesigned moms dress mad completely modern 's...</td>\n",
       "      <td>we redesign mom dress mad complet modern she '...</td>\n",
       "      <td>0</td>\n",
       "      <td>209</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5634070</th>\n",
       "      <td>H0RIamZu0B0Ei0P4aeh3sQ</td>\n",
       "      <td>qskILQ3k0I_qcCMI-k6_QQ</td>\n",
       "      <td>jals67o91gcrD4DC81Vk6w</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Latest addition to services from ICCU is Apple...</td>\n",
       "      <td>2014-12-17 21:45:20</td>\n",
       "      <td>Latest addition services ICCU Apple Pay ICCU d...</td>\n",
       "      <td>latest addit servic iccu appl pay iccu debit c...</td>\n",
       "      <td>3</td>\n",
       "      <td>322</td>\n",
       "      <td>21</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5634071</th>\n",
       "      <td>shTPgbgdwTHSuU67mGCmZQ</td>\n",
       "      <td>Zo0th2m8Ez4gLSbHftiQvg</td>\n",
       "      <td>2vLksaMmSEcGbjI5gywpZA</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>This spot offers a great, affordable east week...</td>\n",
       "      <td>2021-03-31 16:55:10</td>\n",
       "      <td>spot offers great affordable east weekend padd...</td>\n",
       "      <td>thi spot offer great afford east weekend paddl...</td>\n",
       "      <td>1</td>\n",
       "      <td>397</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5634072</th>\n",
       "      <td>YNfNhgZlaaCO5Q_YJR4rEw</td>\n",
       "      <td>mm6E4FbCMwJmb7kPDZ5v2Q</td>\n",
       "      <td>R1khUUxidqfaJmcpmGd4aw</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>This Home Depot won me over when I needed to g...</td>\n",
       "      <td>2019-12-30 03:56:30</td>\n",
       "      <td>Home Depot needed get lot demential lumber see...</td>\n",
       "      <td>thi home depot i need get lot dementi lumber i...</td>\n",
       "      <td>3</td>\n",
       "      <td>467</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      review_id                 user_id  \\\n",
       "5634063  KlHxcAifUF5zDKpJCBrRsw  7ziWZULyiZv2TesYNMFf4g   \n",
       "5634064  PVRvzY0NxSU-fiK3JOXX7w  cTozFTTWjlFYc3yusdbZmA   \n",
       "5634065  E5sXn_qdPX5qRxUEApmiuA  Mc4C7fVY0sEcD-U5eOA2Og   \n",
       "5634066  MIXdpbbCTRAVdi8RiMjwdg  s67G457QlHSvk5RjOMN91w   \n",
       "5634067  Sm8-QDsuQfik-QuhRYT5bw  QIXYkyAbTvgePUOH0-cRFg   \n",
       "5634068  wD5ZWao_vjyT2h4xmGam8Q  7L7GL5Pi2cf8mbm2Dpw4zw   \n",
       "5634069  zHZ-A1qyKDEgyZMDaD--wg  _XVdmFWSgTN6YlojUxixTA   \n",
       "5634070  H0RIamZu0B0Ei0P4aeh3sQ  qskILQ3k0I_qcCMI-k6_QQ   \n",
       "5634071  shTPgbgdwTHSuU67mGCmZQ  Zo0th2m8Ez4gLSbHftiQvg   \n",
       "5634072  YNfNhgZlaaCO5Q_YJR4rEw  mm6E4FbCMwJmb7kPDZ5v2Q   \n",
       "\n",
       "                    business_id  stars  useful  funny  cool  \\\n",
       "5634063  qQO7ErS_RAN4Vs1uX0L55Q      4       1      0     0   \n",
       "5634064  uMVOtr16r1ELu46pWr4HCQ      1       0      0     0   \n",
       "5634065  ZNfph3_VzRLTOemk9Tmzaw      3       5      0     1   \n",
       "5634066  58MJvmfo5hyfBbvkr54sFA      5       1      0     0   \n",
       "5634067  Bh4b8wJRR_ggH7JvpCO7CQ      2       0      0     0   \n",
       "5634068  e_E-jq9mwm7wk75k7Yi-Xw      5       1      0     1   \n",
       "5634069  6WaI-IN8ql0xpEKlb4q8tg      5       1      0     0   \n",
       "5634070  jals67o91gcrD4DC81Vk6w      5       1      2     1   \n",
       "5634071  2vLksaMmSEcGbjI5gywpZA      5       2      1     2   \n",
       "5634072  R1khUUxidqfaJmcpmGd4aw      4       1      0     0   \n",
       "\n",
       "                                                      text  \\\n",
       "5634063  ice cream! ice cream sodas, sundaes!! \\n\\nwant...   \n",
       "5634064  Just average Thai food tonight. Bangkok has al...   \n",
       "5634065  This gym gets the job done. \\n\\nUpsides: \\nChi...   \n",
       "5634066  Great gym.  Was in Indy for 4 days on business...   \n",
       "5634067  The front desk staff is very good and they hav...   \n",
       "5634068  It is very rare for a restaurant to be this go...   \n",
       "5634069  We redesigned my moms dress and mad it complet...   \n",
       "5634070  Latest addition to services from ICCU is Apple...   \n",
       "5634071  This spot offers a great, affordable east week...   \n",
       "5634072  This Home Depot won me over when I needed to g...   \n",
       "\n",
       "                        date  \\\n",
       "5634063  2012-10-21 04:08:40   \n",
       "5634064  2022-01-18 06:42:59   \n",
       "5634065  2016-01-25 21:45:34   \n",
       "5634066  2022-01-18 15:24:44   \n",
       "5634067  2008-09-23 02:52:37   \n",
       "5634068  2022-01-17 22:36:01   \n",
       "5634069  2022-01-17 20:59:01   \n",
       "5634070  2014-12-17 21:45:20   \n",
       "5634071  2021-03-31 16:55:10   \n",
       "5634072  2019-12-30 03:56:30   \n",
       "\n",
       "                                              cleaned_text  \\\n",
       "5634063  ice cream ice cream sodas sundaes \\n\\n wanted ...   \n",
       "5634064  average Thai food tonight Bangkok always go Th...   \n",
       "5634065  gym gets job done \\n\\n Upsides \\n Childcare in...   \n",
       "5634066  Great gym   Indy days business local hotel gym...   \n",
       "5634067  front desk staff good couple quality trainers ...   \n",
       "5634068  rare restaurant good categories food ambience ...   \n",
       "5634069  redesigned moms dress mad completely modern 's...   \n",
       "5634070  Latest addition services ICCU Apple Pay ICCU d...   \n",
       "5634071  spot offers great affordable east weekend padd...   \n",
       "5634072  Home Depot needed get lot demential lumber see...   \n",
       "\n",
       "                                      cleaned_text_stemmed  \\\n",
       "5634063  ice cream ice cream soda sunda want share sund...   \n",
       "5634064  just averag thai food tonight bangkok alway go...   \n",
       "5634065  thi gym get job done upsid childcar includ mem...   \n",
       "5634066  great gym wa indi day busi local hotel gym mee...   \n",
       "5634067  the front desk staff good coupl qualiti traine...   \n",
       "5634068  it rare restaur good categori food ambienc ser...   \n",
       "5634069  we redesign mom dress mad complet modern she '...   \n",
       "5634070  latest addit servic iccu appl pay iccu debit c...   \n",
       "5634071  thi spot offer great afford east weekend paddl...   \n",
       "5634072  thi home depot i need get lot dementi lumber i...   \n",
       "\n",
       "         uppercase_word_count  review_length  hour_of_day  numeric_char_count  \\\n",
       "5634063                     2            528            4                   1   \n",
       "5634064                     2            865            6                  14   \n",
       "5634065                     1           1037           21                   4   \n",
       "5634066                     4            388           15                   2   \n",
       "5634067                     3            451            2                   0   \n",
       "5634068                     0            336           22                   0   \n",
       "5634069                     0            209           20                   0   \n",
       "5634070                     3            322           21                   6   \n",
       "5634071                     1            397           16                   1   \n",
       "5634072                     3            467            3                   2   \n",
       "\n",
       "         predicted_label  class  \n",
       "5634063                0      1  \n",
       "5634064                0      1  \n",
       "5634065                0      1  \n",
       "5634066                0      1  \n",
       "5634067                0      1  \n",
       "5634068                0      1  \n",
       "5634069                0      1  \n",
       "5634070                0      1  \n",
       "5634071                0      1  \n",
       "5634072                0      1  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_true_manual_testing.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8dde34f5-a078-499c-92b5-d97b27d693b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_fake' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data_merge \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([\u001b[43mdata_fake\u001b[49m, data_true], axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      2\u001b[0m data_merge\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m10\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_fake' is not defined"
     ]
    }
   ],
   "source": [
    "data_merge = pd.concat([data_fake, data_true], axis = 0)\n",
    "data_merge.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8c70da-124a-44bd-ae3e-88454f334336",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_merge.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093366f0-a6df-47d9-be36-8972d8729d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_merge.drop(['review_id', 'user_id', 'business_id', 'stars', 'useful', 'funny',\n",
    "       'cool', 'date','cleaned_text', 'cleaned_text_stemmed', 'uppercase_word_count', 'review_length',\n",
    "       'hour_of_day', 'numeric_char_count'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11810a5f-c17b-4ea6-9aab-6227516383af",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427ddda5-a1aa-4cea-9eb3-78c0c9a8e9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'data' is a pandas DataFrame\n",
    "data = data.sample(frac=1)  #for random shuffling\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac27c69-c144-4fec-a517-43f2cdcfda40",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f51f29-9aa6-4f29-b717-be21813713ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.reset_index(inplace = True)\n",
    "data.drop(['index'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9a0194-805c-4332-8cae-062e61307514",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3f7285-554b-4ef8-9197-128ee322c8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf4c358-7933-4b3d-be59-91d6d4ca726a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordopt(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\\\[.*?\\\\]', '', text)\n",
    "    text = re.sub('https?://\\\\S+|www\\\\.\\\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\\\w*\\\\d\\\\w*', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9887100-21eb-4616-b9c3-5fa3c4261f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['text'].apply(wordopt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e5524c-7d5c-45fc-95ca-e9f8496f4a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data['text']\n",
    "y = data['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886ee210-e6f9-46e2-bc72-273b8de79888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bd9569-e459-42dc-a53e-0d01690e5de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming x_train, x_test, y_train, and y_test are already defined\n",
    "\n",
    "# Split the original dataset into training and testing subsets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Split the original training dataset into a smaller subset for training\n",
    "x_train_subset, _, y_train_subset, _ = train_test_split(x_train, y_train, train_size=0.2, stratify=y_train, random_state=42)\n",
    "\n",
    "# Initialize TfidfVectorizer with appropriate parameters\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the training subset\n",
    "xv_train_sparse = vectorizer.fit_transform(x_train_subset)\n",
    "\n",
    "# Assuming x_test and y_test are your original test data and labels respectively\n",
    "\n",
    "# Split the original test dataset into a smaller subset for testing\n",
    "x_test_subset, _, y_test_subset, _ = train_test_split(x_test, y_test, test_size=0.2, stratify=y_test, random_state=42)\n",
    "\n",
    "# Transform the test subset\n",
    "xv_test_sparse = vectorizer.transform(x_test_subset)\n",
    "\n",
    "# Convert the sparse matrices to Compressed Sparse Row (CSR) format for efficient computations\n",
    "xv_train_sparse = xv_train_sparse.tocsr()\n",
    "xv_test_sparse = xv_test_sparse.tocsr()\n",
    "\n",
    "# Now you can use xv_train_sparse and xv_test_sparse for further processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a6d5de-d4a5-49a6-9247-c8ff7c298417",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "LR = LogisticRegression()\n",
    "LR.fit(xv_train_sparse, y_train_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb7e8d9-42c7-405f-8a11-2a711aaf9bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_lr = LR.predict(xv_test_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ac0082-c3ea-49e1-8f26-94d4b0cf952c",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR.score(xv_test_sparse, y_test_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a7cca5-474f-47e6-9ff0-e4b4200fa10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test_subset, pred_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdc8cad-d565-40cc-a09c-773298788c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a sparse matrix or array, e.g., using scipy.sparse.csr_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# Generate some example sparse data (replace this with your actual data)\n",
    "xv_train_sparse = csr_matrix(np.random.random((100, 10)))\n",
    "\n",
    "# Assuming y_train is your label array\n",
    "y_train_subset = np.random.randint(0, 2, size=100)\n",
    "\n",
    "DT = DecisionTreeClassifier()\n",
    "\n",
    "# Use getnnz() to get the number of non-zero elements\n",
    "DT.fit(xv_train_sparse, y_train_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a5f794-5ddb-493f-94bf-ddd071100db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# Assuming you have a sparse matrix or array for training\n",
    "xv_train_sparse = csr_matrix(np.random.random((100, 10)))\n",
    "\n",
    "# Assuming y_train is your label array for training\n",
    "y_train_subset = np.random.randint(0, 2, size=100)\n",
    "\n",
    "# Create a DecisionTreeClassifier instance\n",
    "DT = DecisionTreeClassifier()\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "DT.fit(xv_train_sparse, y_train_subset)\n",
    "\n",
    "# Now you can make predictions\n",
    "# Assuming you have a sparse matrix or array for testing\n",
    "xv_test_sparse = csr_matrix(np.random.random((50, 10)))\n",
    "\n",
    "# Make predictions\n",
    "pred_dt = DT.predict(xv_test_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f252584a-dd53-4c8b-a672-3010794efe80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming you have a sparse matrix or array for training\n",
    "xv_train_sparse = csr_matrix(np.random.random((100, 10)))\n",
    "\n",
    "# Assuming y_train is your label array for training\n",
    "y_train_subset = np.random.randint(0, 2, size=100)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "DT = DecisionTreeClassifier()\n",
    "DT.fit(xv_train_sparse, y_train_subset)\n",
    "\n",
    "# Now you can make predictions\n",
    "# Assuming you have a sparse matrix or array for testing\n",
    "xv_test_sparse = csr_matrix(np.random.random((50, 10)))\n",
    "\n",
    "# Assuming y_test is your label array for testing\n",
    "y_test_subset = np.random.randint(0, 2, size=50)\n",
    "\n",
    "# Assuming you want to evaluate on a random subsample\n",
    "subsample_size = 10000  # Adjust the size based on available memory\n",
    "\n",
    "# Ensure subsample_size is not larger than the number of rows in the sparse matrix\n",
    "subsample_size = min(subsample_size, xv_test_sparse.shape[0])\n",
    "\n",
    "# Generate random subsample indices\n",
    "subsample_indices = np.random.choice(xv_test_sparse.shape[0], size=subsample_size, replace=True)\n",
    "\n",
    "# Use shape[0] to get the number of rows in the sparse matrix\n",
    "pred_dt = DT.predict(xv_test_sparse[subsample_indices])\n",
    "\n",
    "# Calculate accuracy score\n",
    "score = accuracy_score(pred_dt, y_test_subset[subsample_indices])\n",
    "\n",
    "print(\"Score:\", score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4233b2a2-540b-48af-80d5-b314c3882757",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test_subset, pred_dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722f7633-750c-4c64-8e99-cb990fca8327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a sparse matrix or array, e.g., using scipy.sparse.csr_matrix\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# Generate some example sparse data (replace this with your actual data)\n",
    "xv_train_sparse = csr_matrix(np.random.random((100, 10)))\n",
    "\n",
    "# Assuming y_train is your label array\n",
    "y_train_subset = np.random.randint(0, 2, size=100)\n",
    "\n",
    "GB = GradientBoostingClassifier()\n",
    "\n",
    "# Use getnnz() to get the number of non-zero elements\n",
    "GB.fit(xv_train_sparse, y_train_subset)\n",
    "#GB.fit(xv_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb088dde-360f-4994-8064-537db6890769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a sparse matrix or array for training\n",
    "xv_train_sparse = csr_matrix(np.random.random((100, 10)))\n",
    "\n",
    "# Assuming y_train is your label array for training\n",
    "y_train_subset = np.random.randint(0, 2, size=100)\n",
    "\n",
    "GB = GradientBoostingClassifier()\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "GB.fit(xv_train_sparse, y_train_subset)\n",
    "\n",
    "# Now you can make predictions\n",
    "# Assuming you have a sparse matrix or array for testing\n",
    "xv_test_sparse = csr_matrix(np.random.random((50, 10)))\n",
    "\n",
    "# Make predictions\n",
    "pred_gb = GB.predict(xv_test_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd66d3d0-5b50-440a-9700-6ee0b8a24570",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming you have a sparse matrix or array for training\n",
    "xv_train_sparse = csr_matrix(np.random.random((100, 10)))\n",
    "\n",
    "# Assuming y_train is your label array for training\n",
    "y_train_subset = np.random.randint(0, 2, size=100)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "GB = GradientBoostingClassifier()\n",
    "GB.fit(xv_train_sparse, y_train_subset)\n",
    "\n",
    "# Now you can make predictions\n",
    "# Assuming you have a sparse matrix or array for testing\n",
    "xv_test_sparse = csr_matrix(np.random.random((50, 10)))\n",
    "\n",
    "# Assuming y_test is your label array for testing\n",
    "y_test_subset = np.random.randint(0, 2, size=50)\n",
    "\n",
    "# Assuming you want to evaluate on a random subsample\n",
    "subsample_size = 10000  # Adjust the size based on available memory\n",
    "\n",
    "# Ensure subsample_size is not larger than the number of rows in the sparse matrix\n",
    "subsample_size = min(subsample_size, xv_test_sparse.shape[0])\n",
    "\n",
    "# Generate random subsample indices\n",
    "subsample_indices = np.random.choice(xv_test_sparse.shape[0], size=subsample_size, replace=True)\n",
    "\n",
    "# Use shape[0] to get the number of rows in the sparse matrix\n",
    "score = accuracy_score(y_test_subset[subsample_indices], pred_gb[subsample_indices])\n",
    "\n",
    "print(\"Score:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d35379-0d64-4fee-be12-782458775428",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test_subset, pred_gb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bc1c4c-60f3-4ba2-92ca-8889ec2c6755",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# Assuming you have a sparse matrix or array for training\n",
    "xv_train_sparse = csr_matrix(np.random.random((100, 10)))\n",
    "\n",
    "# Assuming y_train is your label array for training\n",
    "y_train_subset = np.random.randint(0, 2, size=100)\n",
    "\n",
    "# Create a RandomForestClassifier instance\n",
    "RF = RandomForestClassifier()\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "RF.fit(xv_train_sparse, y_train_subset)\n",
    "\n",
    "# Now you can make predictions\n",
    "# Assuming you have a sparse matrix or array for testing\n",
    "xv_test_sparse = csr_matrix(np.random.random((50, 10)))\n",
    "\n",
    "# Make predictions\n",
    "pred_rf = RF.predict(xv_test_sparse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7ec293-778c-4f54-9f57-ea0bdbd7c092",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming you have a sparse matrix or array for training\n",
    "xv_train_sparse = csr_matrix(np.random.random((100, 10)))\n",
    "\n",
    "# Assuming y_train is your label array for training\n",
    "y_train_subset = np.random.randint(0, 2, size=100)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "RF = RandomForestClassifier()\n",
    "RF.fit(xv_train_sparse, y_train_subset)\n",
    "\n",
    "# Now you can make predictions\n",
    "# Assuming you have a sparse matrix or array for testing\n",
    "xv_test_sparse = csr_matrix(np.random.random((50, 10)))\n",
    "\n",
    "# Assuming y_test is your label array for testing\n",
    "y_test_subset = np.random.randint(0, 2, size=50)\n",
    "\n",
    "# Assuming you want to evaluate on a random subsample\n",
    "subsample_size = 10000  # Adjust the size based on available memory\n",
    "\n",
    "# Ensure subsample_size is not larger than the number of rows in the sparse matrix\n",
    "subsample_size = min(subsample_size, xv_test_sparse.shape[0])\n",
    "\n",
    "# Generate random subsample indices\n",
    "subsample_indices = np.random.choice(xv_test_sparse.shape[0], size=subsample_size, replace=True)\n",
    "\n",
    "# Use shape[0] to get the number of rows in the sparse matrix\n",
    "score = RF.score(xv_test_sparse[subsample_indices], y_test_subset[subsample_indices])\n",
    "\n",
    "print(\"Score:\", score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d8ba00-6d19-4a9f-8af8-b1e3598d08f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test_subset, pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6706767a-340e-40dd-88a6-dfb91ee24fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_label(n):\n",
    "    if n == 0:\n",
    "        return \"Fake Review\"\n",
    "    elif n == 1:\n",
    "        return \"Not A Fake Review\"\n",
    "\n",
    "def manual_testing(review):\n",
    "    testing_review = {\"text\": [review]}\n",
    "    new_def_test = pd.DataFrame(testing_review)\n",
    "    new_def_test[\"text\"] = new_def_test[\"text\"].apply(wordopt)\n",
    "    new_x_test = new_def_test[\"text\"]\n",
    "    new_xv_test = vectorizer.transform(new_x_test)\n",
    "    pred_LR = LR.predict(new_xv_test)\n",
    "    \n",
    "    return print(\"\\n\\nLR Prediction: {} \".format(output_label(pred_LR[0])))\n",
    "\n",
    "\n",
    "    #   pred_DT = DT.predict(new_xv_test)\n",
    "    #  pred_GB = GB.predict(new_xv_test)\n",
    "    # pred_RF = RF.predict(new_xv_test)\n",
    "    \n",
    "#    return print(\"\\n\\nLR Prediction: {} \\nDT Prediction: {} \\nGB Prediction: {} \\nRF Prediction: {}\".format(\n",
    "#       output_label(pred_LR[0]),\n",
    "#       output_label(pred_DT[0]),\n",
    "#       output_label(pred_GB[0]),\n",
    "#       output_label(pred_RF[0])\n",
    "#    ))\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab040869-64e3-4e78-9a07-10194eb28264",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d239ea-a70e-4b68-85e8-309cd0f3803d",
   "metadata": {},
   "outputs": [],
   "source": [
    "review = str(input())\n",
    "manual_testing(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974b98ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
